<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Realtime AI Bot - GA API</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f5f5f5;
    }
    .status-box {
      background: white;
      padding: 15px;
      border-radius: 8px;
      margin: 10px 0;
      border-left: 4px solid #007bff;
    }
    .status-connected { border-left-color: #28a745; }
    .status-error { border-left-color: #dc3545; }
    .status-processing { border-left-color: #ffc107; }
    #debug {
      font-family: monospace;
      font-size: 12px;
      background: #1a1a1a;
      color: #00ff00;
      padding: 10px;
      border-radius: 4px;
      min-height: 60px;
      white-space: pre-wrap;
    }
    .audio-visualizer {
      height: 20px;
      background: #e9ecef;
      border-radius: 10px;
      margin: 10px 0;
      overflow: hidden;
    }
    .audio-level {
      height: 100%;
      background: linear-gradient(90deg, #28a745, #ffc107, #dc3545);
      width: 0%;
      transition: width 0.1s ease;
    }
  </style>
</head>
<body>
<h2>ðŸ¤– Realtime AI Bot - GA API</h2>

<div id="statusBox" class="status-box">
  <strong>Status:</strong> <span id="status">Connecting to WebSocket...</span>
</div>

<div id="audioBox" class="status-box">
  <strong>Audio:</strong> <span id="audioStatus">Initializing...</span>
  <div class="audio-visualizer">
    <div id="audioLevel" class="audio-level"></div>
  </div>
</div>

<div id="aiBox" class="status-box">
  <strong>AI:</strong> <span id="aiStatus">Waiting for connection...</span>
</div>

<div class="status-box">
  <strong>Debug Info:</strong>
  <div id="debug">Initializing GA API connection...</div>
</div>

<script>
const status = document.getElementById("status");
const audioStatus = document.getElementById("audioStatus");
const aiStatus = document.getElementById("aiStatus");
const debugEl = document.getElementById("debug");
const audioLevel = document.getElementById("audioLevel");
const statusBox = document.getElementById("statusBox");
const audioBox = document.getElementById("audioBox");
const aiBox = document.getElementById("aiBox");

let audioContext;
let isAudioProcessing = false;
let audioChunkCount = 0;
let lastAudioTime = 0;

// Update debug display
function updateDebug(message) {
  const timestamp = new Date().toLocaleTimeString();
  debugEl.textContent = `[${timestamp}] ${message}\n` + debugEl.textContent.substring(0, 1000);
}

// Update status box colors
function updateStatus(element, type) {
  element.className = 'status-box ' + type;
}

// Update audio level visualizer
function updateAudioLevel(level) {
  const width = Math.min(100, level * 500); // Scale the level
  audioLevel.style.width = width + '%';
}

// Connect to your server
updateDebug("Connecting to WebSocket server...");
const ws = new WebSocket("wss://avatar-server-yp11.onrender.com/");

ws.onopen = () => {
  status.textContent = "âœ… Connected to WebSocket";
  updateStatus(statusBox, 'status-connected');
  updateDebug("WebSocket connected successfully - waiting for OpenAI...");
  initializeAudio();
};

ws.onclose = () => {
  status.textContent = "âŒ Disconnected from server";
  updateStatus(statusBox, 'status-error');
  updateDebug("Server connection closed");
};

ws.onerror = (err) => {
  status.textContent = "âŒ WebSocket error";
  updateStatus(statusBox, 'status-error');
  updateDebug("WebSocket error: " + JSON.stringify(err));
  console.error("WebSocket error:", err);
};

// Handle ALL responses from OpenAI GA API
ws.onmessage = (event) => {
  try {
    const msg = JSON.parse(event.data);
    console.log("Received GA API event:", msg.type, msg);
    
    if (msg.type === "ready") {
      aiStatus.textContent = "âœ… Ready and listening";
      updateStatus(aiBox, 'status-connected');
      updateDebug("OpenAI GA API connected and ready");
    }
    // UPDATED FOR GA API: response.output_audio.delta
    else if (msg.type === "response.output_audio.delta" && msg.delta) {
      aiStatus.textContent = "ðŸ”Š Speaking...";
      updateStatus(aiBox, 'status-processing');
      updateDebug("AI audio response received from GA API");
      playAudioChunk(msg.delta);
    } 
    // UPDATED FOR GA API: response.output_audio_transcript.delta
    else if (msg.type === "response.output_audio_transcript.delta" && msg.delta) {
      aiStatus.textContent = "ðŸ’¬ " + msg.delta;
      updateDebug("AI transcript: " + msg.delta);
      console.log("AI is saying:", msg.delta);
    }
    else if (msg.type === "response.done") {
      aiStatus.textContent = "âœ… Response completed";
      updateStatus(aiBox, 'status-connected');
      updateDebug("AI response completed");
    }
    else if (msg.type === "session.updated") {
      updateDebug("OpenAI GA API session configured");
    }
    else if (msg.type === "input_audio_buffer.committed") {
      updateDebug("Audio committed to OpenAI GA API");
    }
    else if (msg.type === "error") {
      aiStatus.textContent = "âŒ AI Error";
      updateStatus(aiBox, 'status-error');
      updateDebug("GA API Error: " + msg.error);
      console.error("Server error:", msg.error);
    }
    else {
      updateDebug("Received GA API event: " + msg.type);
    }
    
  } catch (error) {
    console.error("Error parsing message:", error);
    updateDebug("Parse error: " + error.message);
  }
};

function playAudioChunk(base64Audio) {
  try {
    if (!audioContext) {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    // Convert base64 to ArrayBuffer
    const audioData = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0));
    
    // Create audio buffer (OpenAI uses 24kHz sample rate)
    const audioBuffer = audioContext.createBuffer(1, audioData.length / 2, 24000);
    const channelData = audioBuffer.getChannelData(0);
    
    // Convert Int16 to Float32 for Web Audio API
    const dataView = new DataView(audioData.buffer);
    for (let i = 0; i < channelData.length; i++) {
      const int16 = dataView.getInt16(i * 2, true); // little-endian
      channelData[i] = int16 / 32768.0; // Convert to float
    }
    
    // Play the audio
    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContext.destination);
    source.start();
    
    updateDebug("âœ… Played AI audio: " + audioData.length + " bytes");
    
  } catch (error) {
    console.error("âŒ Error playing audio:", error);
    updateDebug("Audio play error: " + error.message);
  }
}

// Better audio conversion function
function float32ToPCM16(float32Array) {
  const pcm16 = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    const s = Math.max(-1, Math.min(1, float32Array[i]));
    pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
  }
  // Convert to base64
  const uint8 = new Uint8Array(pcm16.buffer);
  let binary = '';
  for (let i = 0; i < uint8.length; i++) {
    binary += String.fromCharCode(uint8[i]);
  }
  return btoa(binary);
}

// Initialize audio processing
async function initializeAudio() {
  try {
    audioStatus.textContent = "ðŸŽ¤ Requesting microphone access...";
    updateStatus(audioBox, 'status-processing');
    updateDebug("Initializing audio system for GA API...");
    
    // Get meeting audio stream (automatically provided by Recall.ai)
    const mediaStream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        sampleRate: 24000, // Match OpenAI's expected sample rate
        channelCount: 1,
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false
      }
    });
    
    audioStatus.textContent = "âœ… Microphone access granted";
    updateStatus(audioBox, 'status-connected');
    updateDebug("Microphone access granted - starting audio processing for GA API");
    
    // Initialize audio context
    audioContext = new AudioContext({ sampleRate: 24000 });
    const source = audioContext.createMediaStreamSource(mediaStream);
    
    // Use ScriptProcessor for audio processing
    const processor = audioContext.createScriptProcessor(4096, 1, 1);
    
    processor.onaudioprocess = (event) => {
      if (isAudioProcessing || ws.readyState !== WebSocket.OPEN) return;
      isAudioProcessing = true;
      
      try {
        const inputData = event.inputBuffer.getChannelData(0);
        
        // Calculate audio level for visualization
        let sum = 0;
        for (let i = 0; i < inputData.length; i++) {
          sum += Math.abs(inputData[i]);
        }
        const average = sum / inputData.length;
        updateAudioLevel(average);
        
        // Check if there's actual audio (not silence)
        let hasAudio = false;
        for (let i = 0; i < inputData.length; i++) {
          if (Math.abs(inputData[i]) > 0.01) {
            hasAudio = true;
            break;
          }
        }
        
        // Only send if audio is above threshold (speech detected)
        if (hasAudio && average > 0.02) {
          const audioBase64 = float32ToPCM16(inputData);
          ws.send(JSON.stringify({ 
            type: "meeting_audio", 
            audio: audioBase64 
          }));
          audioChunkCount++;
          lastAudioTime = Date.now();
          
          // Update audio status periodically
          if (audioChunkCount % 10 === 0) {
            audioStatus.textContent = `âœ… Streaming audio (${audioChunkCount} chunks)`;
            updateDebug(`Audio chunk ${audioChunkCount} sent to GA API (volume: ${Math.round(average * 100)}%)`);
          }
        }
      } catch (error) {
        console.error("Audio processing error:", error);
        updateDebug("Audio processing error: " + error.message);
      }
      
      isAudioProcessing = false;
    };

    source.connect(processor);
    processor.connect(audioContext.destination);
    
    updateDebug("âœ… Audio processing started successfully for GA API");
    console.log("âœ… Audio streaming started for GA API");

  } catch (error) {
    console.error("âŒ Error accessing meeting audio:", error);
    audioStatus.textContent = "âŒ Failed to get meeting audio";
    updateStatus(audioBox, 'status-error');
    updateDebug("Microphone error: " + error.message);
  }
}

// Test audio playback
function testAudioPlayback() {
  if (!audioContext) {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
  }
  
  const oscillator = audioContext.createOscillator();
  oscillator.type = 'sine';
  oscillator.frequency.setValueAtTime(440, audioContext.currentTime);
  oscillator.connect(audioContext.destination);
  oscillator.start();
  oscillator.stop(audioContext.currentTime + 0.1);
  
  updateDebug("Audio context test completed");
}

// Test after a short delay
setTimeout(() => {
  testAudioPlayback();
}, 1000);

// Periodic status update
setInterval(() => {
  if (ws.readyState === WebSocket.OPEN) {
    const now = new Date().toLocaleTimeString();
    const timeSinceAudio = Date.now() - lastAudioTime;
    const statusMsg = timeSinceAudio < 5000 ? "Active" : "Waiting for speech";
    updateDebug(`Heartbeat - ${now} - Audio chunks: ${audioChunkCount} - Status: ${statusMsg}`);
  }
}, 30000);

// Reset audio level visualizer when no audio
setInterval(() => {
  const timeSinceAudio = Date.now() - lastAudioTime;
  if (timeSinceAudio > 1000) {
    updateAudioLevel(0);
  }
}, 100);
</script>
</body>
</html>
