<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Meeting Assistant</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 600px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f5f5f5;
      text-align: center;
    }
    .status-container {
      background: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      margin: 20px 0;
    }
    .status-item {
      margin: 15px 0;
      padding: 10px;
      border-radius: 5px;
      font-size: 16px;
    }
    .status-connecting { background: #fff3cd; color: #856404; }
    .status-connected { background: #d1edff; color: #004085; }
    .status-listening { background: #d4edda; color: #155724; }
    .status-speaking { background: #f8d7da; color: #721c24; }
    .status-error { background: #f8d7da; color: #721c24; }
    
    .audio-visualizer {
      height: 20px;
      background: #e9ecef;
      border-radius: 10px;
      margin: 20px 0;
      overflow: hidden;
    }
    .audio-level {
      height: 100%;
      background: linear-gradient(90deg, #28a745, #ffc107, #dc3545);
      width: 0%;
      transition: width 0.1s ease;
    }
    
    h1 {
      color: #333;
      margin-bottom: 10px;
    }
    
    .ai-response {
      background: #f8f9fa;
      border-left: 4px solid #007bff;
      padding: 15px;
      margin: 15px 0;
      border-radius: 5px;
      min-height: 20px;
      font-style: italic;
    }
  </style>
</head>
<body>
  <h1>ü§ñ AI Meeting Assistant</h1>
  
  <div class="status-container">
    <div id="connectionStatus" class="status-item status-connecting">
      üîÑ Connecting to server...
    </div>
    
    <div id="audioStatus" class="status-item status-connecting">
      üé§ Initializing audio...
    </div>
    
    <div class="audio-visualizer">
      <div id="audioLevel" class="audio-level"></div>
    </div>
    
    <div id="aiStatus" class="status-item status-connecting">
      ‚è≥ Waiting for AI connection...
    </div>
    
    <div id="aiResponse" class="ai-response">
      AI responses will appear here...
    </div>
  </div>

<script>
const connectionStatus = document.getElementById("connectionStatus");
const audioStatus = document.getElementById("audioStatus");
const aiStatus = document.getElementById("aiStatus");
const aiResponse = document.getElementById("aiResponse");
const audioLevel = document.getElementById("audioLevel");

let audioContext;
let isAudioProcessing = false;
let currentAudioSource = null;

// Update audio level visualizer
function updateAudioLevel(level) {
  const width = Math.min(100, level * 500);
  audioLevel.style.width = width + '%';
}

// Connect to your server
const ws = new WebSocket("wss://avatar-server-yp11.onrender.com/");

ws.onopen = () => {
  connectionStatus.textContent = "‚úÖ Connected to server";
  connectionStatus.className = "status-item status-connected";
  
  console.log("‚úÖ Connected to server - waiting for OpenAI session");
  initializeAudio();
};

ws.onclose = () => {
  connectionStatus.textContent = "‚ùå Disconnected from server";
  connectionStatus.className = "status-item status-error";
};

ws.onerror = (err) => {
  connectionStatus.textContent = "‚ùå Connection error";
  connectionStatus.className = "status-item status-error";
};

// Handle responses from OpenAI
ws.onmessage = (event) => {
  try {
    const msg = JSON.parse(event.data);
    
    if (msg.type === "session.updated") {
      aiStatus.textContent = "‚úÖ AI Ready - Listening...";
      aiStatus.className = "status-item status-listening";
      console.log("‚úÖ OpenAI session ready");
    }
    else if (msg.type === "response.interrupt") {
      // Stop any current audio playback immediately
      stopCurrentAudio();
      aiStatus.textContent = "‚èπÔ∏è AI Interrupted - Listening...";
      aiStatus.className = "status-item status-listening";
      aiResponse.textContent = "AI responses will appear here...";
      console.log("‚èπÔ∏è AI response interrupted by user");
    }
    else if (msg.type === "response.complete") {
      // Play complete response when OpenAI finishes
      aiStatus.textContent = "üîä AI Speaking...";
      aiStatus.className = "status-item status-speaking";
      aiResponse.textContent = "AI: " + msg.transcript;
      console.log(`üéØ Playing complete response: ${msg.audioBuffers.length} audio chunks`);
      playCompleteAudio(msg.audioBuffers);
    }
    else if (msg.type === "input_audio_buffer.speech_started") {
      console.log("üé§ User started speaking");
    }
    else if (msg.type === "input_audio_buffer.speech_stopped") {
      console.log("üé§ User stopped speaking - OpenAI will auto-process");
    }
    else if (msg.type === "input_audio_buffer.committed") {
      console.log("‚úÖ OpenAI auto-committed audio");
    }
    else if (msg.type === "error") {
      aiStatus.textContent = "‚ùå AI Error";
      aiStatus.className = "status-item status-error";
      console.error("OpenAI error:", msg.error);
    }
    
  } catch (error) {
    console.error("Error parsing message:", error);
  }
};

// Stop current audio playback immediately
function stopCurrentAudio() {
  if (currentAudioSource) {
    try {
      currentAudioSource.stop();
      currentAudioSource = null;
    } catch (error) {
      // Ignore errors from stopping already finished audio
    }
  }
}

// Play complete audio response
async function playCompleteAudio(audioBuffers) {
  if (!audioContext) {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
  }
  
  // Stop any current playback first
  stopCurrentAudio();
  
  try {
    // Combine all audio buffers into one
    const combinedAudioData = await combineAudioBuffers(audioBuffers);
    
    // Create and play the combined audio buffer
    const audioBuffer = audioContext.createBuffer(1, combinedAudioData.length, 24000);
    audioBuffer.getChannelData(0).set(combinedAudioData);
    
    currentAudioSource = audioContext.createBufferSource();
    currentAudioSource.buffer = audioBuffer;
    currentAudioSource.connect(audioContext.destination);
    currentAudioSource.onended = () => {
      aiStatus.textContent = "‚úÖ AI Ready - Listening...";
      aiStatus.className = "status-item status-listening";
      currentAudioSource = null;
      console.log("‚úÖ Finished playing complete response");
    };
    currentAudioSource.start();
    
  } catch (error) {
    console.error("Error playing complete audio:", error);
    aiStatus.textContent = "‚úÖ AI Ready - Listening...";
    aiStatus.className = "status-item status-listening";
    currentAudioSource = null;
  }
}

// Combine multiple audio buffers into one
function combineAudioBuffers(audioBuffers) {
  return new Promise((resolve) => {
    try {
      // Calculate total length
      let totalLength = 0;
      const allChunks = [];
      
      for (const base64Audio of audioBuffers) {
        const audioData = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0));
        const chunkLength = audioData.length / 2; // 2 bytes per sample (16-bit)
        totalLength += chunkLength;
        allChunks.push(audioData);
      }
      
      // Create combined Float32Array
      const combined = new Float32Array(totalLength);
      let offset = 0;
      
      for (const audioData of allChunks) {
        const dataView = new DataView(audioData.buffer);
        const chunkLength = audioData.length / 2;
        
        for (let i = 0; i < chunkLength; i++) {
          const int16 = dataView.getInt16(i * 2, true);
          combined[offset + i] = int16 / 32768.0;
        }
        offset += chunkLength;
      }
      
      resolve(combined);
      
    } catch (error) {
      console.error("Error combining audio buffers:", error);
      resolve(new Float32Array(0));
    }
  });
}

// Audio conversion function
function float32ToPCM16(float32Array) {
  const pcm16 = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    const s = Math.max(-1, Math.min(1, float32Array[i]));
    pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
  }
  // Convert to base64
  const uint8 = new Uint8Array(pcm16.buffer);
  let binary = '';
  for (let i = 0; i < uint8.length; i++) {
    binary += String.fromCharCode(uint8[i]);
  }
  return btoa(binary);
}

// Initialize audio processing
async function initializeAudio() {
  try {
    audioStatus.textContent = "üé§ Accessing microphone...";
    audioStatus.className = "status-item status-connecting";
    
    // Get meeting audio stream (automatically provided by Recall.ai)
    const mediaStream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        sampleRate: 24000,
        channelCount: 1,
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false
      }
    });
    
    audioStatus.textContent = "‚úÖ Microphone active - Speak normally";
    audioStatus.className = "status-item status-listening";
    
    // Initialize audio context
    audioContext = new AudioContext({ sampleRate: 24000 });
    const source = audioContext.createMediaStreamSource(mediaStream);
    
    // Use ScriptProcessor for audio processing
    const processor = audioContext.createScriptProcessor(1024, 1, 1);
    
    processor.onaudioprocess = (event) => {
      if (isAudioProcessing || ws.readyState !== WebSocket.OPEN) return;
      isAudioProcessing = true;
      
      try {
        const inputData = event.inputBuffer.getChannelData(0);
        
        // Calculate audio level for visualization
        let sum = 0;
        for (let i = 0; i < inputData.length; i++) {
          sum += Math.abs(inputData[i]);
        }
        const average = sum / inputData.length;
        updateAudioLevel(average);
        
        // Send ALL audio to OpenAI - let OpenAI's VAD handle speech detection
        const audioBase64 = float32ToPCM16(inputData);
        ws.send(JSON.stringify({ 
          type: "input_audio_buffer.append",
          audio: audioBase64 
        }));
        
      } catch (error) {
        console.error("Audio processing error:", error);
      }
      
      isAudioProcessing = false;
    };

    source.connect(processor);
    processor.connect(audioContext.destination);
    
    console.log("‚úÖ Audio streaming started - sending all audio to OpenAI VAD");

  } catch (error) {
    console.error("Error accessing meeting audio:", error);
    audioStatus.textContent = "‚ùå Microphone access failed";
    audioStatus.className = "status-item status-error";
  }
}

// Reset audio level visualizer when no audio
setInterval(() => {
  updateAudioLevel(0);
}, 100);
</script>
</body>
</html>
