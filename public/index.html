<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Meeting Assistant</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 600px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f5f5f5;
      text-align: center;
    }
    .status-container {
      background: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      margin: 20px 0;
    }
    .status-item {
      margin: 15px 0;
      padding: 10px;
      border-radius: 5px;
      font-size: 16px;
    }
    .status-connecting { background: #fff3cd; color: #856404; }
    .status-connected { background: #d1edff; color: #004085; }
    .status-listening { background: #d4edda; color: #155724; }
    .status-speaking { background: #f8d7da; color: #721c24; }
    .status-error { background: #f8d7da; color: #721c24; }
    
    h1 {
      color: #333;
      margin-bottom: 10px;
    }
    
    .ai-response {
      background: #f8f9fa;
      border-left: 4px solid #007bff;
      padding: 15px;
      margin: 15px 0;
      border-radius: 5px;
      min-height: 20px;
      font-style: italic;
    }
  </style>
</head>
<body>
  <h1>ü§ñ AI Meeting Assistant</h1>
  
  <div class="status-container">
    <div id="connectionStatus" class="status-item status-connecting">
      üîÑ Connecting to server...
    </div>
    
    <div id="audioStatus" class="status-item status-connecting">
      üé§ Initializing audio...
    </div>
    
    <div id="aiStatus" class="status-item status-connecting">
      ‚è≥ Waiting for AI connection...
    </div>
    
    <div id="aiResponse" class="ai-response">
      AI responses will appear here...
    </div>
  </div>

<script>
const connectionStatus = document.getElementById("connectionStatus");
const audioStatus = document.getElementById("audioStatus");
const aiStatus = document.getElementById("aiStatus");
const aiResponse = document.getElementById("aiResponse");

let audioContext;
let isAudioProcessing = false;

// Connect to your server
const ws = new WebSocket("wss://avatar-server-yp11.onrender.com/");

ws.onopen = () => {
  connectionStatus.textContent = "‚úÖ Connected to server";
  connectionStatus.className = "status-item status-connected";
  initializeAudio();
};

ws.onclose = () => {
  connectionStatus.textContent = "‚ùå Disconnected from server";
  connectionStatus.className = "status-item status-error";
};

ws.onerror = (err) => {
  connectionStatus.textContent = "‚ùå Connection error";
  connectionStatus.className = "status-item status-error";
};

// Handle responses from OpenAI - SIMPLE
ws.onmessage = (event) => {
  try {
    const msg = JSON.parse(event.data);
    
    if (msg.type === "session.updated") {
      aiStatus.textContent = "‚úÖ AI Ready - Listening...";
      aiStatus.className = "status-item status-listening";
    }
    else if (msg.type === "response.output_audio.delta" && msg.delta) {
      aiStatus.textContent = "üîä AI Speaking...";
      aiStatus.className = "status-item status-speaking";
      // Play audio immediately as it comes - let the browser handle timing
      playAudioChunk(msg.delta);
    } 
    else if (msg.type === "response.output_audio_transcript.delta" && msg.delta) {
      aiResponse.textContent = "AI: " + msg.delta;
    }
    else if (msg.type === "response.done") {
      aiStatus.textContent = "‚úÖ AI Ready - Listening...";
      aiStatus.className = "status-item status-listening";
    }
    else if (msg.type === "error") {
      aiStatus.textContent = "‚ùå AI Error";
      aiStatus.className = "status-item status-error";
      console.error("OpenAI error:", msg.error);
    }
  } catch (error) {
    console.error("Error parsing message:", error);
  }
};

// Simple audio playback - let browser handle the timing
function playAudioChunk(base64Audio) {
  try {
    if (!audioContext) {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    // Convert base64 to ArrayBuffer
    const audioData = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0));
    
    // Create audio buffer
    const audioBuffer = audioContext.createBuffer(1, audioData.length / 2, 24000);
    const channelData = audioBuffer.getChannelData(0);
    
    // Convert Int16 to Float32 (following OpenAI docs example)
    const dataView = new DataView(audioData.buffer);
    for (let i = 0; i < channelData.length; i++) {
      const int16 = dataView.getInt16(i * 2, true);
      channelData[i] = int16 / 32768.0;
    }
    
    // Play immediately - browser will handle queuing
    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContext.destination);
    source.start();
    
  } catch (error) {
    console.error("Error playing audio:", error);
  }
}

// Audio conversion function (from OpenAI docs)
function float32To16BitPCM(float32Array) {
  const buffer = new ArrayBuffer(float32Array.length * 2);
  const view = new DataView(buffer);
  let offset = 0;
  for (let i = 0; i < float32Array.length; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
  }
  return buffer;
}

function base64EncodeAudio(float32Array) {
  const arrayBuffer = float32To16BitPCM(float32Array);
  let binary = '';
  let bytes = new Uint8Array(arrayBuffer);
  const chunkSize = 0x8000; // 32KB chunk size
  for (let i = 0; i < bytes.length; i += chunkSize) {
    let chunk = bytes.subarray(i, i + chunkSize);
    binary += String.fromCharCode.apply(null, chunk);
  }
  return btoa(binary);
}

// Initialize audio processing
async function initializeAudio() {
  try {
    audioStatus.textContent = "üé§ Accessing microphone...";
    audioStatus.className = "status-item status-connecting";
    
    // Get meeting audio stream
    const mediaStream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        sampleRate: 24000,
        channelCount: 1,
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false
      }
    });
    
    audioStatus.textContent = "‚úÖ Microphone active - Speak normally";
    audioStatus.className = "status-item status-listening";
    
    // Initialize audio context
    audioContext = new AudioContext({ sampleRate: 24000 });
    const source = audioContext.createMediaStreamSource(mediaStream);
    
    // Process audio
    const processor = audioContext.createScriptProcessor(1024, 1, 1);
    
    processor.onaudioprocess = (event) => {
      if (isAudioProcessing || ws.readyState !== WebSocket.OPEN) return;
      isAudioProcessing = true;
      
      try {
        const inputData = event.inputBuffer.getChannelData(0);
        
        // Convert and send audio (following OpenAI docs exactly)
        const audioBase64 = base64EncodeAudio(inputData);
        ws.send(JSON.stringify({ 
          type: "input_audio_buffer.append",
          audio: audioBase64 
        }));
        
      } catch (error) {
        console.error("Audio processing error:", error);
      }
      
      isAudioProcessing = false;
    };

    source.connect(processor);
    processor.connect(audioContext.destination);
    
    console.log("‚úÖ Audio streaming started");

  } catch (error) {
    console.error("Error accessing meeting audio:", error);
    audioStatus.textContent = "‚ùå Microphone access failed";
    audioStatus.className = "status-item status-error";
  }
}
</script>
</body>
</html>
